projects:

    chocosgd:
        name: chocoSGD
        description: Decentralized communication-efficient ML and DL
        tech_desc:
        code:
            type: Lab GitHub
            url: https://github.com/epfml/ChocoSGD
            date_last_commit: 2019-06-07
        tags:
            - Machine Learning
            - Decentralized
        information:
            - type: Paper
              title: Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication
              url: https://arxiv.org/abs/1902.00340
        type: Application
        license: Apache-2.0
        date_added: 2019-07-30
        date_updated: 2020-02-06
        maturity: 1

    cola:
        name: cola
        description: decentralized linear ML
        tech_desc: >
            Decentralized machine learning is a promising emerging paradigm in view of global challenges of data
            ownership and privacy. We consider learning of linear classification and regression models, in the
            setting where the training data is decentralized over many user devices, and the learning algorithm
            must run on-device, on an arbitrary communication network, without a central coordinator. We propose
            COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical
            performance. Our framework overcomes many limitations of existing methods, and achieves communication
            efficiency, scalability, elasticity as well as resilience to changes in data and participating devices.
        code:
            type: Lab GitHub
            url: https://github.com/epfml/cola
            date_last_commit: 2019-01-20
        information:
            - type: Paper
              title: "COLA: Decentralized Linear Learning"
              url: https://arxiv.org/abs/1808.04883
        tags:
            - Machine Learning
            - Decentralized
        language: Python
        license: Apache-2.0
        type: Application
        date_added: 2019-07-30
        date_updated: 2020-02-06

    mlbench:
        name: mlBench
        description: Benchmarking of distributed ML
        layman_desc: >
            Framework for distributed machine learning. Its purpose is to
            improve transparency, reproducibility, robustness, and to provide
            fair performance measures as well as reference implementations,
            helping adoption of distributed machine learning methods both in
            industry and in the academic community.
            Besides algorithm comparison, a main use case is to help the
            selection of hardware (CPU, GPU) used to run AI applications, as
            well as how to connect it into a cluster to get a good
            cost/performance tradeoff.
        code:
            type: Project GitHub
            url: https://github.com/mlbench
            date_last_commit: 2020-03-26
        url: https://mlbench.github.io
        doc: https://mlbench.readthedocs.io/
        tags:
            - Machine Learning
            - Framework
        language: Python
        type: Framework
        license: Apache-2.0
        date_added: 2019-07-30
        date_updated: 2020-04-01
        maturity: 2

    sent2vec:
        name:         sent2vec
        description:  Numerical representations for short texts
        layman_desc: >
            Library that delivers numerical representations (features) for short
            texts or sentences, which can be used as input to any machine
            learning task later on.
        code:
            type: Lab GitHub
            url:  https://github.com/epfml/sent2vec
            date_last_commit: 2019-10-21
        contacts:
            - name:   Martin Jaggi
              email:  martin.jaggi@epfl.ch
        tags:
            - Machine Learning
            - Text
            - Features
        type: Library
        date_added: 2019-03-18
        date_updated: 2020-03-11
        maturity: 1

    powersgd:
        name: PowerSGD
        description: Practical Low-Rank Gradient Compression for Distributed Optimization
        tech_desc: >
            New low-rank gradient compressor based on power iteration that can
            i) compress gradients rapidly, ii) efficiently aggregate the
            compressed gradients using all-reduce, and iii) achieve test
            performance on par with SGD. The proposed algorithm is the only
            method evaluated that achieves consistent wall-clock speedups when
            benchmarked against regular SGD with an optimized communication
            backend. We demonstrate reduced training times for convolutional
            networks as well as LSTMs on common datasets.
        code:
            type: Lab GitHub
            url: https://github.com/epfml/powersgd
            date_last_commit: 2019-11-26
        information:
            - type:   Paper
              title:  'PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization'
              url:    https://arxiv.org/abs/1905.13727
        tags:
            - Machine Learning
            - Distributed Optimization
        language: Python
        type: Application
        license: MIT
        date_added: 2020-05-01
        date_updated: 2020-05-01
